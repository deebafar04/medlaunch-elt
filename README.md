## MedLaunch ELT ‚Äî Healthcare Facility Data Pipeline üè•

### Objective

Build an AWS data pipeline that process healthcare facility records stored in S3 to analyze accreditation data.

---------------------------------------
### Requirements
#### 1. Stage 1 - Data Extraction with Athena
Use Athena SQL to read nested JSON from S3 and select: `facility_id`, `facility_name`, `employee_count`, `number_of_offered_services`, and `expiry_date_of_first_accreditation`. Save the query results back to S3 so later stages can use them.

#### 2. Stage 2 - Data Processing with Python
Write a Python script (boto3) that reads JSON records from S3 and finds facilities with any accreditation expiring within 6 months. Write the filtered records to a different S3 location and include basic logging and error handling.

#### 3. Stage 3 - Event-Driven Processing with Lambda 
Create a Lambda that triggers when a new JSON file lands in S3 and runs an Athena query to count accredited facilities per state. Store the results in S3 and make sure the function handles timeouts and retries.

#### 4. Stage 4 - Workflow Orchestration with Step Functions (design)
Design a Step Functions state machine that starts on new S3 data and calls a Lambda to run the Athena query, waiting for it to finish. On success, copy results to a production path; on failure, send an SNS alert and include retry and error handling.

---------------------------------
### Dataset

I am using **synthetic data** that the repo generates with 'scripts/generate_facilities_data.py'. It writes NDJSON files (one JSON per line) under 'data/batch/<snapshot_date>/'. This keeps the format simple and easy to stream.

#### How many records I used to test**
I generated **76 records** across three snapshots:

* 20 records for '2025-08-10'
* 50 records for '2025-08-11'
* 6 records for '2025-08-13' - this contains bad data to demonstrate testing.

> To reproduce (run from the repo root):
>
> ```
> python scripts/generate_facilities_data.py --n 1000 --chunk 100 --snapshot 2025-08-10
> python scripts/generate_facilities_data.py --n 50 --chunk 10 --snapshot 2025-08-11 --start-index 21
> ```

#### Example JSON record (one line in the .jsonl file)**

```json
{
  "facility_id": "FAC00042",
  "facility_name": "Grand Rapids Medical Center",
  "location": {
    "address": "1234 Oak Street",
    "city": "Grand Rapids",
    "state": "MI",
    "zip": "49503"
  },
  "employee_count": 312,
  "services": ["Primary Care", "Radiology", "Cardiology"],
  "labs": [
    {"lab_name": "Hematology Lab", "certifications": ["CLIA"]}
  ],
  "accreditations": [
    {"accreditation_body": "Joint Commission", "accreditation_id": "JC712", "valid_until": "2025-11-15"},
    {"accreditation_body": "NCQA", "accreditation_id": "NCQA384", "valid_until": "2026-03-30"}
  ],
  "snapshot_date": "2025-08-10"
}
```

#### What the fields mean (quick tour)**

* 'facility_id', 'facility_name': the identifier and display name. IDs start with 'FAC' and are numbered.
* 'location': address info: 'address', `city`, `state` (US two-letter code), and `zip`.
* `employee_count`: whole number of employees; the generator keeps it within a realistic range.
* `services`: small list of offered services (e.g., Primary Care, Radiology).
* `labs`: zero to two lab entries, each with a `lab_name` and one or more `certifications` (e.g., CLIA, CAP).
* `accreditations`: one or two records showing who accredited the facility, an `accreditation_id`, and a `valid_until` date in `YYYY-MM-DD`.
* `snapshot_date`: the data snapshot date; it‚Äôs also used to organize output folders.

#### How the data is stored in AWS
Bucket: medlaunch-elt-datalake
Raw data lives under bronze-raw-ingested-data/, with one folder per day (folder name = snapshot_date).
<img width="1891" height="559" alt="image" src="https://github.com/user-attachments/assets/a50b2e1c-01b9-4039-9aff-3d52314ef9a9" />

#### Note
I am **not** using the sample MedLaunch dataset for this assessment. All records here are generated by the script.

------
### Bucket Set-up
#### Bucket Structure

<img width="1863" height="667" alt="image" src="https://github.com/user-attachments/assets/6e1741ae-6be8-4b2a-843e-df32f528b90e" />

#### 1. bronze-raw-ingested-data/ ‚Äî holds NDJSON file snapshots

#### 2. python-computed-outputs/  ‚Äî holds Lambda (Stage 2) results

#### 3. stage1-athena-parquet-results/ ‚Äî holds curated Parquet tables (Stage 1)

#### 4. stage1-athena-query-results/ ‚Äî holds the metadata results (Stage 1)

#### 5. stage3-athena-parquet-results/ ‚Äî parquet outputs (Stage 3 tables)

#### 6. stage3-athena-query-results/ ‚Äî CSV outputs (Stage 3)

#### Bucket Features

* **Encryption üîê**
   Used S3 encryption with AWS KMS and **our own key**. AWS rotates the key each year. This keeps files safe while stored (‚Äúat rest‚Äù).

* **Access üö´**
  Blocked **all public access** and turned on **versioning** so we can roll back changes. Added a bucket policy that **allows only HTTPS**; non-TLS (HTTP) requests are rejected.

* **Logging üßæ**
  Send S3 **access logs** to a **separate logs bucket**. This gives a record of who accessed what and when (an audit trail).

* **Lifecycle ‚ôªÔ∏è**
  Added rules to **auto-delete short-lived files**. Example: expire **Athena results** after **7‚Äì14 days**, to control cost.

* **Tags üè∑Ô∏è**
  Labelled the bucket with `DataClass=Confidential` and `ContainsPHI=No (Synthetic)`. Clear labels help reviewers and cost/reporting tools.

----------------------
### Implementation
#### Stage 1
**Architecture Diagram**

<img width="761" height="340" alt="image" src="https://github.com/user-attachments/assets/e2d0bd2f-3976-4d3e-a960-93ba2d5e980a" />

**End-to-end flow:**

* NDJSON data lands in S3 under `bronze-raw-ingested-data/<YYYY-MM-DD>/`.
* Glue Data Catalog has a table that describes the JSON structure and the `snapshot_date` layout.
* Athena uses that Glue schema to read the nested JSON without custom code. The data extracted is stored in bronze_facilities_json_np table.
* Run the stage1_facility_metrics_create SQL to select: facility\_id, facility\_name, employee\_count, number\_of\_offered\_services, and the first accreditation‚Äôs expiry date. This SQL refers to the base table bronze_facilities_json_np to retrieve the data.
* The query applies DQ checks (FAC prefix, non-blank name, non-negative counts) and dedupes by `(snapshot_date, facility_id)`.
* Valid rows are written back to S3 as Parquet in stage1-athena-parquet-results/stage1_facility_metrics/, partitioned by `snapshot_date` (curated dataset).
* Run the stage1_facility_metrics_rejects SQL. Invalid rows are written to stage1-athena-parquet-results/stage1_facility_metrics_rejects with a `reject_reason`.
* S3 buckets stay encrypted (KMS), private (no public access), and HTTPS-only; IAM limits Athena to just the prefixes it needs.

#### Stage 2
**Architecture Diagram**

<img width="801" height="645" alt="image" src="https://github.com/user-attachments/assets/5b62f671-f1d1-4ea5-9616-dcbb8f056897" />

**End-to-end flow:**

* Lambda `stage2_expiring_accreditations.py` is invoked with env vars `DATA_BUCKET=medlaunch-elt-datalake` and `WINDOW_DAYS` (default `180`).
* The function lists all input files under `bronze-raw-ingested-data/batch/<YYYY-MM-DD>/*.jsonl` (and `.jsonl.gz`) and pulls `snapshot_date` from the folder name.
* Each file is streamed line-by-line from S3 (gzip supported). No full-file loads; memory stays low.
* For every record, DQ checks are run: `facility_id` starts with `FAC`, `facility_name` not blank, `employee_count` ‚â• 0, and `accreditations` present.
* For each accreditation, we parse `valid_until`; if it falls within `today ‚Üí today + WINDOW_DAYS`, emit one ‚Äúexpiring-soon‚Äù row with facility, location, accreditor, `valid_until`, `days_until_expiry`, `run_date`, and `snapshot_date`.
* If `valid_until` is missing/invalid (or the record fails any DQ), we write a quarantine line capturing `reject_reason`, file key, and `snapshot_date`.
* After scanning all inputs, we write a single CSV to `python-computed-outputs/expiring-soon/run_date=<YYYY-MM-DD>/expiring_soon.csv`.
* After scanning all inputs, we write per-file rejects to `python-computed-outputs/quarantine/<snapshot_date>/<file>_rejects.jsonl` so bad rows are easy to review without polluting outputs.
* Emit structured INFO logs (files processed, rows emitted/rejected, output paths) to Amazon CloudWatch Logs for audit and troubleshooting.
* S3 is KMS-encrypted, private (Block Public Access), and HTTPS-only; the Lambda role has least-privilege IAM: `ListBucket`/`GetObject` on bronze and `PutObject` on the output prefixes.

#### Stage 3
**Architecture Diagram**

<img width="1015" height="481" alt="image" src="https://github.com/user-attachments/assets/bf147115-ddf7-469d-885c-eda0428dde5c" />

**End-to-end flow:**

* A new JSON file lands in `s3://medlaunch-elt-datalake/bronze-raw-ingested-data/batch/<YYYY-MM-DD>/*.jsonl`. This upload **automatically triggers** the Stage-3 Lambda.
* The Lambda quickly checks the event. If the file isn‚Äôt a bronze `.jsonl`, it **ignores** it.
* It creates a unique **run id** (`run_ts`) for this invocation and reads its settings (DB name, scratch/output paths).
* The Lambda asks **Athena** to run SQL that **counts accredited facilities per state** (and another SQL that lists **rejects** with reasons).
* Athena writes those results into two Parquet-backed tables in S3 (stored under `stage3-athena-parquet-results/...`), so they‚Äôre easy to query later.
* The Lambda then **checks how many rows** were inserted for this `run_ts`.
* If rows exist, it **exports** each result set to easy-to-download **CSV** files:

  * Processed counts ‚Üí `stage3-athena-query-results/Processed Results/accredited_facilities_per_state/run_ts=<run_ts>/`
  * Rejects ‚Üí `stage3-athena-query-results/Rejected Results/accredited_facilities_per_state/run_ts=<run_ts>/`
* If that `run_ts` folder already exists, the Lambda **skips the export** to avoid duplicates (idempotent behavior).
* Throughout the run, it writes simple JSON logs (start, query ids, row counts, output paths) to **CloudWatch Logs** so you can see exactly what happened.
* If Athena takes too long, the Lambda **cancels** the query and fails fast. Normal Lambda **retries** handle transient issues.
* Security is on by default: S3 is **KMS-encrypted**, bucket access is private and **HTTPS-only**, and the Lambda‚Äôs IAM role can read the bronze path and write only to its own output prefixes.

-------------------
