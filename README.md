## MedLaunch ELT ‚Äî Healthcare Facility Data Pipeline üè•

#### Demo Link: https://www.loom.com/share/ee0df09496cf4511bdabecaff55440e0?sid=e7351bb0-c3b4-4386-ac3b-bc207dc200e3
### Objective

Build an AWS data pipeline that process healthcare facility records stored in S3 to analyze accreditation data.

---------------------------------------
### Requirements
#### 1. Stage 1 - Data Extraction with Athena
Use Athena SQL to read nested JSON from S3 and select: `facility_id`, `facility_name`, `employee_count`, `number_of_offered_services`, and `expiry_date_of_first_accreditation`. Save the query results back to S3 so later stages can use them.

#### 2. Stage 2 - Data Processing with Python
Write a Python script (boto3) that reads JSON records from S3 and finds facilities with any accreditation expiring within 6 months. Write the filtered records to a different S3 location and include basic logging and error handling.

#### 3. Stage 3 - Event-Driven Processing with Lambda 
Create a Lambda that triggers when a new JSON file lands in S3 and runs an Athena query to count accredited facilities per state. Store the results in S3 and make sure the function handles timeouts and retries.

#### 4. Stage 4 - Workflow Orchestration with Step Functions (design)
Design a Step Functions state machine that starts on new S3 data and calls a Lambda to run the Athena query, waiting for it to finish. On success, copy results to a production path; on failure, send an SNS alert and include retry and error handling.

---------------------------------
### Dataset

I am using **synthetic data** that the repo generates with 'scripts/generate_facilities_data.py'. It writes NDJSON files (one JSON per line) under 'data/batch/<snapshot_date>/'. This keeps the format simple and easy to stream.

#### How many records I used to test
I generated **76 records** across three snapshots:

* 20 records for '2025-08-10'
* 50 records for '2025-08-11'
* 6 records for '2025-08-13' - this contains bad data to demonstrate testing.

> To reproduce (run from the repo root):
>
> ```
> python scripts/generate_facilities_data.py --n 1000 --chunk 100 --snapshot 2025-08-10
> python scripts/generate_facilities_data.py --n 50 --chunk 10 --snapshot 2025-08-11 --start-index 21
> ```

#### Example JSON record (one line in the .jsonl file)

```json
{
  "facility_id": "FAC00042",
  "facility_name": "Grand Rapids Medical Center",
  "location": {
    "address": "1234 Oak Street",
    "city": "Grand Rapids",
    "state": "MI",
    "zip": "49503"
  },
  "employee_count": 312,
  "services": ["Primary Care", "Radiology", "Cardiology"],
  "labs": [
    {"lab_name": "Hematology Lab", "certifications": ["CLIA"]}
  ],
  "accreditations": [
    {"accreditation_body": "Joint Commission", "accreditation_id": "JC712", "valid_until": "2025-11-15"},
    {"accreditation_body": "NCQA", "accreditation_id": "NCQA384", "valid_until": "2026-03-30"}
  ],
  "snapshot_date": "2025-08-10"
}
```

#### What the fields mean (quick tour)

* 'facility_id', 'facility_name': the identifier and display name. IDs start with 'FAC' and are numbered.
* 'location': address info: 'address', `city`, `state` (US two-letter code), and `zip`.
* `employee_count`: whole number of employees; the generator keeps it within a realistic range.
* `services`: small list of offered services (e.g., Primary Care, Radiology).
* `labs`: zero to two lab entries, each with a `lab_name` and one or more `certifications` (e.g., CLIA, CAP).
* `accreditations`: one or two records showing who accredited the facility, an `accreditation_id`, and a `valid_until` date in `YYYY-MM-DD`.
* `snapshot_date`: the data snapshot date; it‚Äôs also used to organize output folders.

#### How the data is stored in AWS
Bucket: medlaunch-elt-datalake
Raw data lives under bronze-raw-ingested-data/, with one folder per day (folder name = snapshot_date).
<img width="1891" height="559" alt="image" src="https://github.com/user-attachments/assets/a50b2e1c-01b9-4039-9aff-3d52314ef9a9" />

#### Note
I am **not** using the sample MedLaunch dataset for this assessment. All records here are generated by the script.

------
### Bucket Set-up
#### Bucket Structure

<img width="1863" height="667" alt="image" src="https://github.com/user-attachments/assets/6e1741ae-6be8-4b2a-843e-df32f528b90e" />

#### 1. bronze-raw-ingested-data/ ‚Äî holds NDJSON file snapshots

#### 2. python-computed-outputs/  ‚Äî holds Lambda (Stage 2) results

#### 3. stage1-athena-parquet-results/ ‚Äî holds curated Parquet tables (Stage 1)

#### 4. stage1-athena-query-results/ ‚Äî holds the metadata results (Stage 1)

#### 5. stage3-athena-parquet-results/ ‚Äî parquet outputs (Stage 3 tables)

#### 6. stage3-athena-query-results/ ‚Äî CSV outputs (Stage 3)

#### Bucket Features

* **Encryption üîê**
   Used S3 encryption with AWS KMS and **our own key**. AWS rotates the key each year. This keeps files safe while stored (‚Äúat rest‚Äù).

* **Access üö´**
  Blocked **all public access** and turned on **versioning** so we can roll back changes. Added a bucket policy that **allows only HTTPS**; non-TLS (HTTP) requests are rejected.

* **Logging üßæ**
  Send S3 **access logs** to a **separate logs bucket**. This gives a record of who accessed what and when (an audit trail).

* **Lifecycle ‚ôªÔ∏è**
  Added rules to **auto-delete short-lived files**. Example: expire **Athena results** after **7‚Äì14 days**, to control cost.

* **Tags üè∑Ô∏è**
  Labelled the bucket with `DataClass=Confidential` and `ContainsPHI=No (Synthetic)`. Clear labels help reviewers and cost/reporting tools.

----------------------
### Implementation
#### Stage 1
**Architecture Diagram**

<img width="761" height="340" alt="image" src="https://github.com/user-attachments/assets/e2d0bd2f-3976-4d3e-a960-93ba2d5e980a" />

**End-to-end flow:**

* NDJSON data lands in S3 under `bronze-raw-ingested-data/<YYYY-MM-DD>/`.
* Glue Data Catalog has a table that describes the JSON structure and the `snapshot_date` layout.
* Athena uses that Glue schema to read the nested JSON without custom code. The data extracted is stored in bronze_facilities_json_np table.
* Run the stage1_facility_metrics_create SQL to select: facility\_id, facility\_name, employee\_count, number\_of\_offered\_services, and the first accreditation‚Äôs expiry date. This SQL refers to the base table bronze_facilities_json_np to retrieve the data.
* The query applies DQ checks (FAC prefix, non-blank name, non-negative counts) and dedupes by `(snapshot_date, facility_id)`.
* Valid rows are written back to S3 as Parquet in stage1-athena-parquet-results/stage1_facility_metrics/, partitioned by `snapshot_date` (curated dataset).
* Run the stage1_facility_metrics_rejects SQL. Invalid rows are written to stage1-athena-parquet-results/stage1_facility_metrics_rejects with a `reject_reason`.
* S3 buckets stay encrypted (KMS), private (no public access), and HTTPS-only; IAM limits Athena to just the prefixes it needs.

#### Stage 2
**Architecture Diagram**

<img width="801" height="645" alt="image" src="https://github.com/user-attachments/assets/5b62f671-f1d1-4ea5-9616-dcbb8f056897" />

**End-to-end flow:**

* Lambda `stage2_expiring_accreditations.py` is invoked with env vars `DATA_BUCKET=medlaunch-elt-datalake` and `WINDOW_DAYS` (default `180`).
* The function lists all input files under `bronze-raw-ingested-data/batch/<YYYY-MM-DD>/*.jsonl` (and `.jsonl.gz`) and pulls `snapshot_date` from the folder name.
* Each file is streamed line-by-line from S3 (gzip supported). No full-file loads; memory stays low.
* For every record, DQ checks are run: `facility_id` starts with `FAC`, `facility_name` not blank, `employee_count` ‚â• 0, and `accreditations` present.
* For each accreditation, we parse `valid_until`; if it falls within `today ‚Üí today + WINDOW_DAYS`, emit one ‚Äúexpiring-soon‚Äù row with facility, location, accreditor, `valid_until`, `days_until_expiry`, `run_date`, and `snapshot_date`.
* If `valid_until` is missing/invalid (or the record fails any DQ), we write a quarantine line capturing `reject_reason`, file key, and `snapshot_date`.
* After scanning all inputs, we write a single CSV to `python-computed-outputs/expiring-soon/run_date=<YYYY-MM-DD>/expiring_soon.csv`.
* After scanning all inputs, we write per-file rejects to `python-computed-outputs/quarantine/<snapshot_date>/<file>_rejects.jsonl` so bad rows are easy to review without polluting outputs.
* Emit structured INFO logs (files processed, rows emitted/rejected, output paths) to Amazon CloudWatch Logs for audit and troubleshooting.
* S3 is KMS-encrypted, private (Block Public Access), and HTTPS-only; the Lambda role has least-privilege IAM: `ListBucket`/`GetObject` on bronze and `PutObject` on the output prefixes.

#### Stage 3
**Architecture Diagram**

<img width="1015" height="481" alt="image" src="https://github.com/user-attachments/assets/bf147115-ddf7-469d-885c-eda0428dde5c" />

**End-to-end flow:**

* A new JSON file lands in `s3://medlaunch-elt-datalake/bronze-raw-ingested-data/batch/<YYYY-MM-DD>/*.jsonl`. This upload **automatically triggers** the Stage-3 Lambda.
* The Lambda quickly checks the event. If the file isn‚Äôt a bronze `.jsonl`, it **ignores** it.
* It creates a unique **run id** (`run_ts`) for this invocation and reads its settings (DB name, scratch/output paths).
* The Lambda asks **Athena** to run SQL that **counts accredited facilities per state** (and another SQL that lists **rejects** with reasons).
* Athena writes those results into two Parquet-backed tables in S3 (stored under `stage3-athena-parquet-results/...`), so they‚Äôre easy to query later.
* The Lambda then **checks how many rows** were inserted for this `run_ts`.
* If rows exist, it **exports** each result set to easy-to-download **CSV** files:

  * Processed counts ‚Üí `stage3-athena-query-results/Processed Results/accredited_facilities_per_state/run_ts=<run_ts>/`
  * Rejects ‚Üí `stage3-athena-query-results/Rejected Results/accredited_facilities_per_state/run_ts=<run_ts>/`
* If that `run_ts` folder already exists, the Lambda **skips the export** to avoid duplicates (idempotent behavior).
* Throughout the run, it writes simple JSON logs (start, query ids, row counts, output paths) to **CloudWatch Logs** so you can see exactly what happened.
* If Athena takes too long, the Lambda **cancels** the query and fails fast. Normal Lambda **retries** handle transient issues.
* Security is on by default: S3 is **KMS-encrypted**, bucket access is private and **HTTPS-only**, and the Lambda‚Äôs IAM role can read the bronze path and write only to its own output prefixes.

#### Stage 4 ‚Äî Planned Workflow (Step Functions)
**Note** Due to time constraint, this stage is not implemented. However, the below are the steps depicting how I would have implemented.

**End-to-end flow**

* A new file lands at `bronze-raw-ingested-data/batch/<YYYY-MM-DD>/*.jsonl`. This upload fires an **EventBridge** event.
* An **EventBridge rule** starts the **Step Functions** state machine (only for `.jsonl` under that prefix).
* The workflow calls our **Stage-3 Lambda**. It runs Athena, inserts rows, and exports CSVs for this run (tagged with `run_ts`).
* The workflow waits for the Lambda to finish and checks row counts. If zero, it stops. If rows exist, it continues.
* It **copies results** from the staging prefixes to **production** prefixes (one path for processed, one for rejects).
* On success, it ends. On failure, it sends an **SNS** alert with the bucket, key, error, and `run_ts`.

**State machine (states):**

* **ValidateEvent** ‚Äî Guardrail. Only continue for bronze `.jsonl` keys.
* **RunStage3** ‚Äî Invoke the existing Lambda (sync). Get `run_ts`, row counts, and the CSV target prefixes.
* **CheckCounts** ‚Äî If both processed and rejects are zero, end; otherwise continue.
* **CopyProcessed** ‚Äî Copy everything under
  `stage3-athena-query-results/Processed Results/accredited_facilities_per_state/run_ts=<run_ts>/`
  to a production prefix, e.g.
  `prod/processed/accredited_facilities_per_state/run_ts=<run_ts>/`.
* **CopyRejects** ‚Äî Same copy step for the rejects prefix.
* **Success** ‚Äî End of workflow.
* **NotifyFailure** ‚Äî If any step throws, publish an **SNS** message with context (bucket, key, error, `run_ts`).


-------------------
### Run/Execution Instructions

#### Stage 1

**Run order**

1. `stage1_facility_metrics_create.sql` ‚Äî create curated table (CTAS).
2. `stage1_facility_metrics_rejects.sql` ‚Äî create rejects table (CTAS).
3. `stage1_facility_metrics_insert.sql` ‚Äî append **new** snapshot\_date partitions to curated.
4. `stage1_facility_metrics_rejects_insert.sql` ‚Äî append **new** partitions to rejects.

> On day-2 and later: run **only** steps 3 and 4 to load the new dates.

**Option 1: Athena console (quickest)**

* Open **Athena ‚Üí Query editor**.
* Set **Database** = `medlaunch_db`, **Workgroup** = `primary`.
* Run the files in the order above (adjust file paths if needed).
* Outputs:

  * Curated Parquet ‚Üí `s3://<bucket>/stage1-athena-parquet-results/stage1_facility_metrics/`
  * Rejects Parquet ‚Üí `s3://<bucket>/stage1-athena-parquet-results/stage1_facility_metrics_rejects/`

**Opetion 2: Windows CMD (AWS CLI)**

> Replace `<BUCKET>` 

```cmd
set BUCKET=<YOUR_BUCKET_NAME>
set DB=medlaunch_db
set WG=primary

:: 1) Create curated table
aws athena start-query-execution ^
  --query-string file://sql/athena/stage1_data_extarction_with_athena/stage1_facility_metrics_create.sql ^
  --query-execution-context Database=%DB% ^
  --work-group %WG% ^
  --result-configuration OutputLocation=s3://%BUCKET%/stage1-athena-parquet-results/

:: 2) Create rejects table
aws athena start-query-execution ^
  --query-string file://sql/athena/stage1_data_extarction_with_athena/stage1_facility_metrics_rejects.sql ^
  --query-execution-context Database=%DB% ^
  --work-group %WG% ^
  --result-configuration OutputLocation=s3://%BUCKET%/stage1-athena-parquet-results/

:: 3) Insert only NEW partitions into curated
aws athena start-query-execution ^
  --query-string file://sql/athena/stage1_data_extarction_with_athena/stage1_facility_metrics_insert.sql ^
  --query-execution-context Database=%DB% ^
  --work-group %WG% ^
  --result-configuration OutputLocation=s3://%BUCKET%/stage1-athena-parquet-results/

:: 4) Insert only NEW partitions into rejects
aws athena start-query-execution ^
  --query-string file://sql/athena/stage1_data_extarction_with_athena/stage1_facility_metrics_rejects_insert.sql ^
  --query-execution-context Database=%DB% ^
  --work-group %WG% ^
  --result-configuration OutputLocation=s3://%BUCKET%/stage1-athena-parquet-results/
```

**Notes**

* These scripts assume inputs live under `bronze-raw-ingested-data/batch/<YYYY-MM-DD>/` and tables are partitioned by `snapshot_date`.
* If you re-ingest the same date and need to refresh, delete that date‚Äôs partition in S3 (both curated and rejects) **before** re-running the insert scripts.
* Keep a lifecycle rule on `stage1-athena-query-results/` (7‚Äì14 days) to control cost.
---
#### Stage 2

**Option 1: Trigger from CMD (manual run)**


```cmd
set FN=<STAGE2_LAMBDA_NAME>
aws lambda invoke --function-name %FN% --payload "{}" out.json --log-type Tail
type out.json
```
---

#### Stage 3 ‚Äî Lambda (event-driven Athena + CSV export)

**Auto-trigger**

Just upload a new JSONL file to the bronze path; the **S3 event** fires the Lambda.

---
### Tech stack ‚öôÔ∏è

**Cloud**

* **Amazon S3** ‚Äî data lake (bronze, curated, exports)
* **AWS Glue Data Catalog** ‚Äî schemas for bronze and curated tables
* **Amazon Athena** ‚Äî SQL on S3 (CTAS/INSERT/UNLOAD)
* **AWS Lambda (Python 3.12)** ‚Äî Stage 2 filter, Stage 3 event runner
* **AWS Step Functions** ‚Äî orchestration (Stage 4, planned)
* **Amazon EventBridge** ‚Äî S3 event ‚Üí Step Functions trigger (planned)
* **AWS KMS** ‚Äî SSE-KMS encryption for buckets/outputs
* **AWS IAM** ‚Äî least-privilege access
* **Amazon CloudWatch Logs** ‚Äî run logs and troubleshooting

**Languages & libs**

* **Python 3.12** ‚Äî Lambda + utilities

  * **boto3** ‚Äî S3/Athena/KMS/IAM calls
  * **pandas** (via awswrangler) ‚Äî DataFrame ‚Üí Parquet
  * **Faker** ‚Äî synthetic data generator
* **SQL (Athena)** ‚Äî Stage 1 and Stage 3 queries

**IaC & tooling**

* **AWS CDK (Python)** ‚Äî buckets, keys, Glue, Lambdas, notifications
* **AWS CLI** ‚Äî quick triggers and uploads

**Data formats**

* **NDJSON** (bronze input)
* **Parquet (Snappy)** (curated tables)
* **CSV** (human-friendly exports)

#### Cleanup

We have deployed the code using CDK. 

```
cdk destroy
```

This removes the stack resources (buckets must be empty or set to auto-delete in dev).

#### Thanks

Thanks for reading. If something‚Äôs unclear, file an issue with a screenshot or log snippet (deebafar04@gmail.com) and I‚Äôll tighten the docs.
